import re
import requests
from bs4 import BeautifulSoup
import datetime
import random
from model import transformer
import mojimoji
import model_utils
import pickle
from asari.api import Sonar
from googletrans import Translator


class Reply:
    def __init__(
        self, tokenizer_path, hparams_path, model_paths, back_tr=False, text_pp=True
    ):
        self.tokenizer_path = tokenizer_path
        self.tokenizer = self.load_pickle(tokenizer_path)
        self.hparams_path = hparams_path
        self.hparams = self.load_pickle(hparams_path)
        self.model_paths = model_paths
        self.models = self.load_model(model_paths)
        self.back_tr = back_tr
        self.text_pp = text_pp
        self.negative_reactions = ["ğŸ¥º", "ğŸ¥´", "ğŸ˜­"]
        self.positive_reactions = ["ğŸ¥³", "ğŸ¥°", "ğŸ˜˜"]
        self.sonar = Sonar()
        self.translator = Translator()

    def reply_help(self):
        help = "ä½¿ç”¨å¯èƒ½ãªã‚³ãƒãƒ³ãƒ‰ã§ã™\n/image [word] : wordã®ç”»åƒæ¤œç´¢\n/news : æŠ€è¡“ç³»ãƒ‹ãƒ¥ãƒ¼ã‚¹\n/tenki : ç¾åœ¨ã®å¤©æ°—\n/clear : ãƒ­ã‚°å‰Šé™¤\n/kill : å¼·åˆ¶çµ‚äº†\n"
        return help

    def reply_image(self, content):
        word = content.split()[-1]
        url = f"http://image.search.yahoo.co.jp/search?n=60&p={word}&search.x=1"
        html = requests.get(url)
        links = re.findall("https://[\w/:%#\$&\?\(\)~\.\+\-]+[jpgpng]{3}", html.text)
        clean_urls = []
        for link in links:
            if "yimg" in link:
                continue
            else:
                clean_urls.append(link)
        if len(clean_urls) == 0:
            link = links[0]
        else:
            link = random.choice(clean_urls)
        text = "å–å¾—ã—ãŸç”»åƒã§ã™\n" + link
        return text

    def reply_news(self):
        url = "https://qiita.com/"
        res = requests.get(url)
        soup = BeautifulSoup(res.text, "html.parser")
        items = soup.find_all(class_="css-qrra2n")
        text = "ä»Šæ—¥ã®æŠ€è¡“ç³»ãƒ‹ãƒ¥ãƒ¼ã‚¹ã§ã™\n"
        text += "############################################\n"
        for i in range(min(len(items), 5)):
            item = items[i]
            text += "ã‚¿ã‚¤ãƒˆãƒ«ï¼š" + item.text + "\n"
            text += "URLï¼š" + item.get("href") + "\n"
            text += "############################################\n"
        return text

    def reply_forecast(self):
        url = "https://tenki.jp/forecast/5/27/5310/24205/1hour.html"
        r = requests.get(url)
        html = r.text.encode(r.encoding)
        soup = BeautifulSoup(html, "lxml")
        n_hour = max(abs(int(datetime.datetime.now().hour) - 1), 0)
        hour = soup.select(".hour > td")[n_hour].text.strip()
        weather = soup.select(".weather > td")[n_hour].text.strip()
        temperature = soup.select(".temperature > td")[n_hour].text.strip()
        prob_precip = soup.select(".prob-precip > td")[n_hour].text.strip()
        precipitation = soup.select(".precipitation > td")[n_hour].text.strip()
        humidity = soup.select(".humidity > td")[n_hour].text.strip()
        wind_blow = soup.select(".wind-blow > td")[n_hour].text.strip()

        w_info = (
            "ä»Šæ—¥ã®å¤©æ°—ã§ã™" + "\n"
            "æ™‚åˆ»         ï¼š "
            + hour
            + "æ™‚"
            + "\n"
            + "å¤©æ°—         ï¼š "
            + weather
            + "\n"
            + "æ°—æ¸©(C)      ï¼š "
            + temperature
            + "\n"
            + "é™æ°´ç¢ºç‡(%)  ï¼š "
            + prob_precip
            + "\n"
            + "é™æ°´é‡(mm/h) ï¼š "
            + precipitation
            + "\n"
            + "æ¹¿åº¦(%)      ï¼š "
            + humidity
            + "\n"
            + "é¢¨å‘         ï¼š "
            + wind_blow
            + "\n"
            + "é¢¨é€Ÿ(m/s)    ï¼š "
            + weather
            + "\n"
        )

        return w_info

    def reply_chat(self, user_name, content):
        content_pp = re.sub("<.+>", "", content).lstrip()
        text = mojimoji.han_to_zen(content_pp)
        text = model_utils.predict(self.hparams, self.models, self.tokenizer, text)
        reaction = None
        response = self.sonar.ping(text=content_pp)
        negative = response["classes"][0]["confidence"]
        positive = response["classes"][1]["confidence"]
        if negative >= 0.95:
            reaction = random.choice(self.negative_reactions)
        if positive >= 0.97:
            reaction = random.choice(self.positive_reactions)
        if self.back_tr:
            text = self.translator.translate(text, src="ja", dest="en").text
            text = self.translator.translate(text, src="en", dest="ja").text
        if self.text_pp:
            text = self.text_postprocessing(text, user_name)
        return text, reaction

    def text_postprocessing(self, text, user_name):
        text = text.replace("ã‚ãªãŸ", user_name + "ã•ã‚“")
        text = text.replace("ãŠå‰", user_name + "ã•ã‚“")
        text = text.replace("å¾¡å‚ã•ã‚“", user_name + "ã•ã‚“")
        text = text.replace("å…ˆè¼©", user_name + "ã•ã‚“")
        text = text.replace("ãŠå¬¢æ§˜", user_name + "ã•ã‚“")
        text = text.replace("ã”ä¸»äººæ§˜", user_name + "ã•ã‚“")
        text = text.replace("é•·ç”·", user_name + "ã•ã‚“")
        text = text.replace("ç”·æ€§", user_name + "ã•ã‚“")
        text = text.replace("å¥³æ€§", user_name + "ã•ã‚“")
        text = text.replace("å°‘å¥³", user_name + "ã•ã‚“")
        text = text.replace("ç”·ã•ã‚“", user_name + "ã•ã‚“")
        text = text.replace("ç”·", user_name + "ã•ã‚“")
        text = text.replace("å¥³", user_name + "ã•ã‚“")
        text = text.replace("è²´æ§˜", user_name + "ã•ã‚“")
        text = text.replace("ãŠçˆ¶ã•ã‚“", user_name + "ã•ã‚“")
        text = text.replace("ãŠå…„ã¡ã‚ƒã‚“", user_name + "ã•ã‚“")
        text = text.replace("ãƒˆãƒ¢B", user_name + "ã•ã‚“")
        text = text.replace("ãƒˆãƒƒãƒ¢ï¼¢", user_name + "ã•ã‚“")
        text = text.replace("é¹¿ç›®ã•ã‚“", user_name + "ã•ã‚“")
        text = text.replace("ãŠå§‰ã¡ã‚ƒã‚“", user_name + "ã•ã‚“")
        text = text.replace("åŸç”°ç¾ä¸–", user_name + "ã•ã‚“")
        text = text.replace("è²´æ–¹", user_name + "ã•ã‚“")
        text = text.replace("èœã€…ã•ã‚“", user_name + "ã•ã‚“")
        text = text.replace("æ¯ã•ã‚“", user_name + "ã•ã‚“")
        text = text.replace("æœªå¤®", user_name + "ã•ã‚“")
        text = text.replace("ã‚¢ãƒƒã‚³ã•ã‚“", user_name + "ã•ã‚“")
        text = text.replace("å§‰ã¡ã‚ƒã‚“", user_name + "ã•ã‚“")
        text = text.replace("ä¿ºæ§˜", "ç§")
        text = text.replace("ä¿º", "ç§")
        text = text.replace("åƒ•", "ç§")
        text = text.replace("ç§ã•ã‚“", "ç§")
        text = text.replace("ã•ã‚“ã•ã‚“", "ã•ã‚“")
        text = text.replace("eï¼Ÿ", "ãˆï¼Ÿ")
        text = re.sub("(ãã®ã€)+", "ãã®ã€", text)
        text = re.sub("(ã“ã‚Œã€)+", "ã“ã‚Œã€", text)
        text = re.sub("(ä»Šæ—¥ã€)+", "ä»Šæ—¥ã€", text)
        text = re.sub("(ã‚ã®ã€)+", "ã‚ã®ã€", text)
        text = re.sub("(ã‚ã‚Œã€)+", "ã‚ã‚Œã€", text)
        text = re.sub("(ã“ã®ã€)+", "ã“ã®ã€", text)
        text = re.sub("(ãšã£ã¨ã€)+", "ãšã£ã¨ã€", text)
        text = re.sub("(ã‚“ãƒ¼ã€)+", "ã‚“ãƒ¼ã€", text)
        text = re.sub("(æ¯æ—¥ã€)+", "æ¯æ—¥ã€", text)
        text = re.sub("(æ¯æ—¥)+", "æ¯æ—¥", text)
        text = re.sub("(ç§ã€)+", "ç§ã€", text)
        text = re.sub("ç§ã¯ã€ç§ã¯", "ç§ã¯", text)
        if "ï¼ï¼" in text or ">>" in text:
            text = "ã‚ã€ã‚ã®"
        if text == "ã‚ã‚ã‚ã‚":
            text = "ãã†ã§ã™ã‹"
        if text == "ãã†ã ãª":
            text = "ãã†ã§ã™ã‹"
        return text

    def load_pickle(self, path):
        with open(path, "rb") as handle:
            obj = pickle.load(handle)
        return obj

    def load_model(self, paths):
        models = []
        for i in range(len(paths)):
            path = paths[i]
            model = transformer(self.hparams)
            model.load_weights(path)
            models.append(model)
        return models
